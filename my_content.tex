\section{Introduction}
In contemporary radiation therapy, photon intensity modulated radiation therapy (IMRT) stands as a pivotal technique utilized to attain precise and conformal dose distributions within target volumes[add citation].
This achievement owes its realization chiefly to the advent of the multileaf collimator (MLC)[add citation].

Radiation therapy is now a reliable treatment for oncology [add citation].
Despite this consensus, the way to deliver radiotherapy for its best result remain very dependent upon doctors.
Moreover, it appears that there is a large variability across centers[add citation?].

To achieve the best treatment, doctors need to solve a complex inverse mathematical optimization problem with multiples trade-offs[add citation].
There is a lack of standardized prioritization of constraints make the optimization a real challenge[add citation].
The standard procedure nowadays is to manually guide computer optimization: dosimetrists manually update the settings of an optimizing software (so called Treatment Planning System)[add citation].

There has been many tries to create a metric that quantify the quality of a treatment plan: Normal tissue complication probabilities (NTCP), Target coverage, Conformity index, Heterogeneity index (non-exhaustive list)[add citations].\label{metrics}
However, none of them has been able to satisfy all radio-oncologists, and the only reliable way of assessing a plan for doctors is to check out the dose-volume histograms (DVHs) them-self.

As a result, Pareto surface exploration are doomed to failure due to the lack of impartial quantitative measurement for a particular plan[add citation].
Other meta-optimization techniques are similarly bounded, for the same reason[add citation].
An extra challenge to attend for those is the fact that not all cases have the same "difficulty".
Hence, for an "easy" case, doctors will require an excellent dose (in terms of the metrics mentioned above), while they can be more permissive for "harder" cases.
This make the acceptability of a plan hard to define in general.

Reinforcement learning is a machine learning paradigm concerned with training agents to make sequential decisions in dynamic environments.
Through a process of trial and error guided by rewards or penalties, agents learn to optimize their actions to achieve long-term objectives.
It appears that the decisions taken by dosimetrists when performing the optimization of a treatment can be formalized as a reinforcement learning problem.
Moreover, dosimetrists can guide the TPS towards an acceptable plan, but they usually struggle explaining their decision while interacting with the TPS.
This suggest the use of deep reinforcement learning, over expert base methods.

\section{Materials and Methods}
We introduce a new paradigm in reinforcement learning (RL), based on the evaluation of states, rather than the reward.

\subsection{Reinforcement Learning Reward}
In classical RL, we want $V(S_t) = R_t + \gamma V(S_{t+1})$
(so the update is $V(S_t) \leftarrow (1-\alpha) V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right]$).
In the context of dose optimization, the reward $R_t$ is defined as $R_t = \mathcal{E}(S_{t+1}) - \mathcal{E}(S_t)$.
Where $\mathcal{E}$ is a function that evaluates the quality of a state (such that higher is better; if lower is better, then swap $s_t$ and $S_{t+1}$).

The evaluation $\mathcal{E}$ can one, or a mixture of the metrics mentions in \ref{metrics}.
This setup may leverage knowledge about which actions to perform, instead of guessing randomly as a meta optimizer would do.
We can hope to gain some computation time.



\subsection{Reward-Free Reinforcement Learning}


\section{Results}

\section{Discussion}



\section*{Appendix}

\subsection*{Synthetic phantom patients}
As this is very new and ongoing research, we generated synthetic phantom patients and associated trust-able clinical dose.
In future work, we hope to be able to apply this technique to real cases.

\subsection*{Clinical dose}

\subsection*{Optimization}

\subsection*{Evaluation}
