\section{Introduction}
% combine first two §?
In contemporary radiation therapy, photon intensity modulated radiation therapy (IMRT) is a pivotal technique to attain precise and conformal dose distributions within target volumes \cite{xu_comparison_2017}.
This achievement owes its realization to the advent of the multileaf collimator (MLC) \cite{galvin_characterization_1993}.

Radiation therapy is now a reliable treatment for oncology \cite{valentini_survival_2009}.
Despite this consensus, the way to deliver radiotherapy for its best result remains very dependent upon doctors.
Moreover, there appears to be a large variability across physicians and centres, but in terms of 3D structures contouring and irradiation, constrains priorities \cite{variability_2021}.

To achieve the best treatment, doctors must solve a complex inverse mathematical optimization problem with multiple trade-offs \cite{oelfke_inverse_2001} \cite{webb_physical_2003}.
However a lack of standardized prioritization of constraints makes the optimization a real challenge.
The standard procedure nowadays is to guide computer optimization manually: dosimetrists manually update the settings of an optimizing software so-called Treatment Planning System (TPS) \cite{planification_website}.

There have been many tries to create a metric that quantifies the quality of a treatment plan, such as Normal Tissue Complication Probabilities (NTCP), target coverage, conformity index, and heterogeneity index, among others/to name a few \cite{lyman_normal_1992} \cite{li_input_2022}\label{metrics}.
However, they have yet to satisfy all radio-oncologists, and the only reliable way to assess a doctor's plan is to evaluate the dose-volume histograms (DVHs) themselves.

As a result, Pareto surface exploration is unsuitable due to the lack of impartial quantitative measurement for a particular plan \cite{huang_pareto_2021}.
Other meta-optimization techniques are similarly bounded for the same reason \cite{wu_optimization_2001} \cite{xing_optimization_1999}.
An extra challenge to attend for those is the fact that not all cases have the same "difficulty."
Hence, for an "easy" case, doctors will require an excellent dose (in terms of the metrics mentioned above), while they can be more permissive for "harder" cases.
This makes the acceptability of a plan hard to define in general.

Reinforcement learning (RL) is a machine learning paradigm that trains agents to make sequential decisions in dynamic environments \cite{brooks_what_2021}.
Through trial and error guided by rewards or penalties, agents learn to optimize their actions to achieve long-term objectives.
The decisions taken by dosimetrists when optimizing treatment can be formalized as a RL problem.
Moreover, dosimetrists can guide the TPS towards an acceptable plan but usually struggle to explain their decision while interacting with the TPS.
The difficulty in explaining why certain decisions are taken suggests using deep RL over expert-based methods.
This setup is similar to image recognition, where one can say a picture represents a car or a boat but struggles to explain why.

We sought to leverage deep learning to learn the actions a dosimetrist takes when optimizing a dose using a treatment planning system.
The study’s primary hypothesis was that all the information needed to decide what weights should be changed in the objective function used by the optimizer relies on the Dose Volume Histograms (DVHs).
This assumption is supported by the fact that dosimetrists almost solely use those DVHs plots.
We trained an agent that takes as an input the DVHs of the current optimized dose, and predicts the evaluation of possible weights changes.

We allowed our-self to use the dose distributions of previously treated patients to train our model.
However, we consider that access to the exact actions taken by human dosimetrists on the TPS is unavailable (as clinics do not usually store this data; only the final plan is held).
This data availability suggests the use of RL.

\section{Materials and Methods}
% We introduce a new paradigm in RL based on evaluating states rather than the reward.
We introduce a new paradigm for reward-based dosimetrist RL agents.
This new reward system aims to mimic human-optimized doses better.

\subsection{Reinforcement Learning Reward}

\begin{figure*}
	\centering
	\includegraphics[width=0.9\textwidth]{reward.pdf}
	\caption{Classical reinforcement learning reward for automatic dosimetry.}
	\label{fig:reward_fig}
\end{figure*}

In classical RL, we want $V(S_t) = R_t + \gamma V(S_{t+1})$
(so the update is $V(S_t) \leftarrow (1-\alpha) V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right]$).
In the context of dose optimization, the reward $R_t$ is defined as $R_t = \mathcal{E}(S_{t+1}) - \mathcal{E}(S_t)$.
Where $\mathcal{E}$ is a function that evaluates the quality of a state (such that higher is better; if lower is better, then swap $s_t$ and $S_{t+1}$).

The evaluation $\mathcal{E}$ can be one or a mixture of the metrics mentioned in the introduction (Section \ref{metrics}) \cite{shen_hierarchical_2021} \cite{shen_intelligent_2019} \cite{moreau_reinforcement_2021}.
This setup may leverage knowledge about which actions to perform instead of guessing randomly as a meta-optimizer would do.
We can hope to gain some computation time.

However, this technique does not use past plans; it only needs the optimizer inputs (CT, structures contours).
We propose using the availability of past treatment plans to catch better the complexity of decisions made by dosimetrists and better match their expectations of a fully automatic treatment planning system.

As developed in previous work, we can derive a distance between dose plans \cite{paul_dubois_novel_2024}.
If we consider the clinical dose of past cases (used for training) as the best achievable one, we can evaluate a dose plan by computing its distance from the clinical dose plan.

Let $D_t$ be the dose associated with $S_t$, and $D_C$ the clinical dose.
We then define $\mathcal{E}(S_t) = \mathcal{D}(D_t, D_C)$.
Since in that case, lower is better, we will define the reward as $$R_t = \mathcal{E}(S_t) - \mathcal{E}(S_{t+1}) = \mathcal{D}(D_t, D_C) - \mathcal{D}(D_{t+1}, D_C).$$
This reward can be interpreted as the "distance gained to the clinical dose".


%\subsection{Reward-Free Reinforcement Learning}
%
%Since the reward is based on an evaluation of the state, one may drop all the reward machinery and directly use the state evaluation.
%This allows us to capture the signal better: it is not embedded in a reward system, and the network can learn faster.
%
%The reason why the reward system is used in many games (such as go or chess) is that state evaluation is challenging, if not impossible; there is no function that, given the board, gives the quality of white/black's position.
%A reward is only given at the end of a game, usually $+1$ for winning and $-1$ for losing.
%The state's evaluation is deduced by the RL agent while learning to maximize the reward.
%
%However, in our dosimetrist case, we have precisely such an evaluation function.
%Embedding this evaluation in a reward by defining the reward as the evaluation difference turns out to dilute the signal, and the agent learns slower.
%
%By predicting the next state evaluation instead of using the Bellman optimality equation, we lose the ability to make short-term concessions in order to obtain large rewards later on.
%In our case, making short-term concessions would mean that we trigger the weights in a way that the optimizer performs worse, but such that triggering a little more makes the optimizer perform better.
%Given the problem we are solving (optimization of a radiotherapy dose), this is highly unlikely.
%
%Because optimizers have the terrible habit of getting stuck in a local minimum, we can not use the last optimization result as a "warm start" after modifying optimization weights.
%Hence, restarting optimization from zero is forced after every weight modification.
%
%Given the statement of the optimization problem, and the fact that we can not use a warm start, making short-term compromises does not make much sense.
%It is, therefore, safe to predict the next state evaluation directly, without the use of the Bellman optimality equation.


\subsection{Architecture}
We use a dense neural network, taking the DVHs and the current normalized weight values as inputs.
It outputs the $Q(s, a)$ Value for each possible action $a$.
Dense layers are very prone to overfitting.
In order to force the network to actually predict the following evaluation for each possible action, without overfitting, we incorporated a bottleneck in the network (figure \ref{fig:architecture}).
Compressing the information stops the network from overfitting.
Networks with such architecture show very little difference between training and validation sets (see figure \ref{fig:losses_training}).

\begin{figure*}
	\centering
	\includegraphics[width=6cm]{architecture_all_actions.pdf}
	\hspace{0.5cm}
	\includegraphics[width=8cm]{losses-distance.pdf}
	\caption{Neural network architecture and Loss evolution while training.}
	\label{fig:architecture}
	\label{fig:losses_training}
\end{figure*}

\subsection{Avoiding Off-Distribution}
We generated a training set of over 125k actions (this took five days on an NVIDIA GeForce GTX 1080).
Despite this relatively large dataset, we have not explored exhaustively the state-actions space, and the network still gets off distribution.
This can easily be spotted when the predicted distance is negative; we choose to ignore those predictions.
In fact, we ignore all outlier predictions.
The justification is that our set of actions is limited, no action will suddenly drastically improve the plan.
It is the combination of several sequential actions that allows good plan optimization.
Therefore, while testing, we choose the action with the best prediction, while passing the outlier test just mentioned.

\section{Results}
Figure \ref{fig:distance} show how the distance between our RL agent's perform over five steps on 30 test patients (unseen during the training).
A lower distance is interpreted as a better dose (as it is closer to the best dose: the clinical one).
\begin{figure*}
	\centering
	\includegraphics[width=0.6\textwidth]{DistanceToClinicalDose.pdf}
	\caption{Average distance between RL agent's dose and clinical dose.}
	\label{fig:distance}
\end{figure*}


\subsection{Quantitative Results}
The network was able to converge on the training data, and validation showed some minor overfitting.
For testing, we generated 30 brand new cases that we again manually optimized.
We then used the RL model to perform the optimization of those 30 unseen cases.
On average, our model was able to reduce the dose distance with manually optimized dose by a factor of $2.5$ (from $2.2$ at iteration $0$ to $0.91$ at iteration $4$), as shown in the table \ref{table:results} ($^*$ means lower is better; $^\dagger$ means higher is better).

\begin{center}
	\begin{tabular}{| c || c | c | c |} 
		\hline
		Agent $\backslash$ Metric & Mean Final Distance$^*$ & Homogeneity Score$^\dagger$ & Conformity Score$^\dagger$ \\ 
		\hline
		RL Distance Score & \textbf{0.612} & 1.871 & 0.406 \\ 
		RL Homogeneity Score & 2.012 & \textbf{4.387} & 0.567 \\
		RL Conformity Score &  1.781  & 4.232 & 0.451 \\
		Meta-optimization & 1.279 & 4.117 & \textbf{0.610} \\	
		\hline
	\end{tabular}
	\label{table:results}
\end{center}


\subsection{Qualitative Results}
Figure \ref{fig:steps} is the detail of the DVHs at each of the five steps of an optimization on one of the test patient (unseen by the agent during the training).
Our model was able to drastically reduce the dose distance with manually optimized dose.
Visual inspection of the DVHs plot can show that the dose optimized by the RL agent is very close to the clinical (manually fine-tuned) one.

\begin{figure*}
	\centering
	\includegraphics[width=0.49\textwidth]{steps/distance-test-w1.pdf}	\includegraphics[width=0.49\textwidth]{steps/distance-test-w2.pdf}	\includegraphics[width=0.49\textwidth]{steps/distance-test-w3.pdf}	\includegraphics[width=0.49\textwidth]{steps/distance-test-w4.pdf}
	\caption{RL Agent DVHs after each action taken on a test (unseen) patient. Solid lines are the agent's dose DVHs; dotted ones are the reference dose DVHs (manually fine-tuned).}
	\label{fig:steps}
\end{figure*}

\section{Discussion}
Our study demonstrates the potential of deep reinforcement learning (RL) for automating radiotherapy treatment plan optimization.
A key strength of our approach is its ability to learn from past treatment plans, capturing the complex decision-making processes of human dosimetrists.
This data-driven approach avoids the limitations of pre-defined metrics, which may not fully capture the nuances of optimal treatment planning.
%Additionally, directly predicting the subsequent state evaluation bypasses the need for reward shaping, which can be challenging in complex optimization tasks.

However, our study also has limitations.
The agent's performance relies on the quality and quantity of available training data.
Cases with limited historical data or complex anatomical features may require additional strategies.
Moreover, while the agent achieves promising results in terms of dose distance reduction, it is not guaranteed that the dose will be clinically acceptable.
Finally,  while this study demonstrates the promise of our RL approach in a controlled setting, extending it to real-world radiotherapy planning necessitates addressing additional complexities and constraints.


Several avenues exist for further research.
Firstly, we plan to investigate strategies for incorporating additional information, such as patient characteristics and anatomical complexities, into the training process.
Secondly, we aim to explore techniques for improving the interpretability of the agent's decision-making process, allowing for better understanding and potential clinical validation.

\section{Conclusion}
Our approach differs from previous RL-based methods for radiotherapy planning in two key aspects.
We avoid relying on pre-defined metrics for evaluation, which can be subjective and limit the agent's ability to learn complex optimization strategies.
% We also directly predict the next state evaluation, eliminating the need for reward shaping and potentially improving learning efficiency.
Compared to traditional meta-optimization approaches, our method leverages past treatment data, potentially leading to more informed decision-making during the optimization process.

This study demonstrates deep RL's feasibility and potential benefits for automating radiotherapy treatment plan optimization.
Our approach, which leverages past treatment data and directly predicts state evaluations, shows promise in achieving significant improvements in efficiency and, potentially, treatment outcomes.
Further research is needed to address limitations, improve interpretability, and ensure safe clinical integration.
We believe this approach has the potential to revolutionize radiotherapy planning, leading to more standardized, efficient, and potentially, improved patient care.

\subsection*{Appendix}
As this is very new and ongoing research, we generated synthetic phantom patients and associated trustable clinical doses.
In future work, we hope to apply this technique to real cases.

\subsubsection*{Synthetic phantom patients}
We generated 130 patients with oval axial section bodies.
We set the body density to water density.
We then added an ellipsoid PTV within the body, with a slightly different density (following $\mathcal{N}(1,0.05)$).
Likewise, we generate five organs gravitating around the PTV, aligned on the axial section.

\begin{figure*}
	\centering
	\label{fig:main_slice-ct}	\includegraphics[height=5cm]{main_slice-ct.pdf}
	\hspace{1cm}
	\label{fig:main_slice-dose}
	\includegraphics[height=5cm]{main_slice-dose.pdf}
%	\hspace{0.1cm}
	\\
	\label{fig:clinical_dvh}
	\includegraphics[width=10cm]{dvh_example.pdf}
	\caption{
		Example of a (generated) patient: \\
		\textit{Left:} Main axial slice (center of the PTV) \textbf{CT}.\\
		\textit{Right:} Main axial slice (center of the PTV) of the \textbf{clinical dose}. \\
		\textit{Bottom:} Associated clinical dose \textbf{DVH}.
	}
	
\end{figure*}

\subsubsection*{Clinical dose}
After generating the patient's CT and structures, we needed to create a reference dose that our agent should mimic.
We manually set weights and performed a standard optimization.
The dose prescription is a standard 80Gy on PTV, and is the same across all patients.


\subsubsection*{Optimization}
We optimize the plan using the LBFGS optimizer (shown to be the most appropriate in \cite{dubois_radiotherapy_2023}).
For each DVH constraint (e.g. for PTV, $D_{95}>80 \ Gy$), we used a linear penalization of the overdose.
