\section{Introduction}
In contemporary radiation therapy, photon intensity modulated radiation therapy (IMRT) stands as a pivotal technique utilized to attain precise and conformal dose distributions within target volumes \cite{xu_comparison_2017}.
This achievement owes its realization chiefly to the advent of the multileaf collimator (MLC) \cite{galvin_characterization_1993}.

Radiation therapy is now a reliable treatment for oncology \cite{valentini_survival_2009}.
Despite this consensus, the way to deliver radiotherapy for its best result remain very dependent upon doctors.
Moreover, it appears that there is a large variability across physicians and centers, bot in terms of 3D structures contouring and irradiation constraints priorities \cite{variability_2021}.

To achieve the best treatment, doctors need to solve a complex inverse mathematical optimization problem with multiples trade-offs \cite{oelfke_inverse_2001} \cite{webb_physical_2003}.
There is a lack of standardized prioritization of constraints make the optimization a real challenge.
The standard procedure nowadays is to manually guide computer optimization: dosimetrists manually update the settings of an optimizing software (so called Treatment Planning System) \cite{planification_website}.

There has been many tries to create a metric that quantify the quality of a treatment plan: Normal tissue complication probabilities (NTCP), Target coverage, Conformity index, Heterogeneity index (non-exhaustive list)[add citations].\label{metrics}
However, none of them has been able to satisfy all radio-oncologists, and the only reliable way of assessing a plan for doctors is to check out the dose-volume histograms (DVHs) them-self.

As a result, Pareto surface exploration are doomed to failure due to the lack of impartial quantitative measurement for a particular plan[add citation].
Other meta-optimization techniques are similarly bounded, for the same reason[add citation].
An extra challenge to attend for those is the fact that not all cases have the same "difficulty".
Hence, for an "easy" case, doctors will require an excellent dose (in terms of the metrics mentioned above), while they can be more permissive for "harder" cases.
This make the acceptability of a plan hard to define in general.

Reinforcement learning is a machine learning paradigm concerned with training agents to make sequential decisions in dynamic environments.
Through a process of trial and error guided by rewards or penalties, agents learn to optimize their actions to achieve long-term objectives.
It appears that the decisions taken by dosimetrists when performing the optimization of a treatment can be formalized as a reinforcement learning problem.
Moreover, dosimetrists can guide the TPS towards an acceptable plan, but they usually struggle explaining their decision while interacting with the TPS.
This suggest the use of deep reinforcement learning, over expert base methods.

\section{Materials and Methods}
We introduce a new paradigm in reinforcement learning (RL), based on the evaluation of states, rather than the reward.

\subsection{Reinforcement Learning Reward}
In classical RL, we want $V(S_t) = R_t + \gamma V(S_{t+1})$
(so the update is $V(S_t) \leftarrow (1-\alpha) V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) \right]$).
In the context of dose optimization, the reward $R_t$ is defined as $R_t = \mathcal{E}(S_{t+1}) - \mathcal{E}(S_t)$.
Where $\mathcal{E}$ is a function that evaluates the quality of a state (such that higher is better; if lower is better, then swap $s_t$ and $S_{t+1}$).

The evaluation $\mathcal{E}$ can be one, or a mixture of the metrics mentions in introduction (Section \ref{metrics}) [add citation].
This setup may leverage knowledge about which actions to perform, instead of guessing randomly as a meta optimizer would do.
We can hope to gain some computation time.

However, this technique is not using the plan used in past cases; it only needs the optimizer inputs (CT, structures contours).
We propose to use the availability of past treatment plans, to better catch the complexity of decision taken by dosimetrists, and match better their expectations of a fully automatic treatment planning system.

As developed in previous work, we can derive a distance between doses plans [add citation].
If we consider the clinical dose of past cases (used for training) as the best achievable one, then we can evaluate a dose plan by computing its distance with the clinical dose plan.

Letting $D_t$ be the dose associated with $S_t$, and $D_C$ the clinical dose.
We then define $\mathcal{E}(S_t) = \mathcal{D}(D_t, D_C)$.
Since in that case, lower is better, we will define the reward as $$R_t = \mathcal{E}(S_t) - \mathcal{E}(S_{t+1}) = \mathcal{D}(D_t, D_C) - \mathcal{D}(D_{t+1}, D_C).$$
This reward can be interpreted as the "distance gained to the clinical dose".


\subsection{Reward-Free Reinforcement Learning}


\section{Results}

\section{Discussion}



\section*{Appendix}

\subsection*{Synthetic phantom patients}
As this is very new and ongoing research, we generated synthetic phantom patients and associated trust-able clinical dose.
In future work, we hope to be able to apply this technique to real cases.

\subsection*{Clinical dose}

\subsection*{Optimization}

\subsection*{Evaluation}

